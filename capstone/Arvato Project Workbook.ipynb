{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To process large dataset\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# To check performance of the operation\n",
    "import time\n",
    "\n",
    "from matplotlib_venn import venn2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Since the dataset is more than 1GB, I've used `c` engine instead of `python` to load data faster. In addition, there are mixed data in column 18 & 19; more specifically some `NaN` values are represented by `X` & `XX`. These values are set as `na_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Demographics data for the general population of Germany\n",
    "azdias = pd.read_csv('Udacity_AZDIAS_052018.csv', na_values=[\"X\", \"XX\"], engine=\"c\")\n",
    "\n",
    "# Demographics data for customers of a mail-order company\n",
    "customers = pd.read_csv('Udacity_CUSTOMERS_052018.csv', na_values=[\"X\", \"XX\"], engine=\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Preliminary Information about the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.select_dtypes(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are three features in 'customers' which are not present in 'azdias'. These features are redundant.\n",
    "customers = customers.drop(columns=[\"CUSTOMER_GROUP\", \"ONLINE_PURCHASE\", \"PRODUCT_GROUP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.select_dtypes(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if both azdias and customers have same features\n",
    "azdias_features = list(azdias.columns)\n",
    "customers_feature = list(customers.columns)\n",
    "set(azdias_features) == set(customers_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Features and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .xlsx file for Attributes description\n",
    "# Skip first row and slice the dataframe to drop first column which are empty cells in excel\n",
    "attributes = pd.read_excel(\"DIAS Attributes - Values 2017.xlsx\", header=1)\n",
    "attributes = attributes.loc[:,[\"Attribute\", \"Value\", \"Meaning\"]].fillna(method='ffill')\n",
    "attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique attributes in 'atrtibutes'\n",
    "attribute_list = list(attributes.Attribute.unique())\n",
    "len(attribute_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** Both `azdias` & `customers` have 366 features while the `attributes` only contains 314 features. The missing or unknown values in the features from `azdias` & `customers` that are also present in `attributes`, can be referred from `attributes` and mapped to `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common and unique features between 'azdias' and 'attributes'\n",
    "common_features = set(azdias_features) & set(attribute_list)\n",
    "unique_azdias_features = set(azdias_features) - set(attribute_list)\n",
    "num_common_features = len(common_features)\n",
    "num_unique_azdias_features = len(unique_azdias_features)\n",
    "print(\"common features: {}, unique azdias features: {}\".format(num_common_features, num_unique_azdias_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features whose attributes are known\n",
    "azdias_subset = azdias.drop(columns=list(unique_azdias_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** `azdias` & `customers` have 272 common features with `attributes`. The features with unknown or no information value will be considered as `NaN`. The steps to do so are listed below:\n",
    "1. From `attributes` _unknown_ or _no information_ values are identified  \n",
    "2. Subset of `attributes` with _unknown_ or _missing values_ was created \n",
    "3. Check if the values in `azdias` or `customers` match with _unknown_ or _missing values_\n",
    "4. The unknown values in `attributes` are assigned `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of attributes with unknown or no information value\n",
    "attributes_unknown_val = attributes[(attributes['Meaning'].str.contains(\"unknown\") | \\\n",
    "                                 attributes['Meaning'].str.contains(\"no \"))]\n",
    "attributes_unknown_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some attributes, the unknown or no information values are list while for other attributes they are integer value. Unknown values for each attributes will be converted to list for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unknown_val = []\n",
    "for attribute in attributes_unknown_val['Attribute'].unique():\n",
    "    val = attributes_unknown_val.loc[attributes['Attribute'] == attribute, 'Value'].astype(\"str\") \\\n",
    "                                                                        .str.cat(sep=',').split(',')\n",
    "    val = list(map(int, val))\n",
    "    unknown_val.append(val)\n",
    "attributes_unknown_val = pd.concat([pd.Series(attributes_unknown_val['Attribute'].unique()),\n",
    "                                    pd.Series(unknown_val)], axis=1)\n",
    "\n",
    "attributes_unknown_val.columns = ['attribute', 'unknown']\n",
    "attributes_unknown_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We know which features are common between azdias and attributes. So we will replace the unknown or \n",
    "# missing information with NaN    \n",
    "for row in attributes_unknown_val.itertuples(index=False):\n",
    "    if row.attribute in azdias_subset.columns.values.tolist():\n",
    "        print(row.attribute)\n",
    "        nan_val = attributes_unknown_val.loc[attributes_unknown_val['attribute'] == row.attribute, 'unknown'].iloc[0]\n",
    "        #print(nan_val)\n",
    "        #na_map = list(map(str.strip, na_map.strip('][').replace(\"'\", '').split(',')))\n",
    "        nan_idx = azdias_subset.loc[:, row.attribute].isin(nan_val)\n",
    "        #print(nan_idx)\n",
    "        azdias_subset.loc[nan_idx, row.attribute] = np.NaN\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "start = time.time()\n",
    "    \n",
    "for row in attributes_unknown_val.itertuples(index=False):\n",
    "    if row.attribute in customers.columns.values.tolist():\n",
    "        print(row.attribute)\n",
    "        nan_val = attributes_unknown_val.loc[attributes_unknown_val['attribute'] == row.attribute, 'unknown'].iloc[0]\n",
    "        #print(nan_val)\n",
    "        #na_map = list(map(str.strip, na_map.strip('][').replace(\"'\", '').split(',')))\n",
    "        nan_idx = customers.loc[:, row.attribute].isin(nan_val)\n",
    "        #print(nan_idx)\n",
    "        customers.loc[nan_idx, row.attribute] = np.NaN\n",
    "    else:\n",
    "        continue        \n",
    "end = time.time()\n",
    "\n",
    "customers.head(20)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of NaN count on each column\n",
    "nan_count_column_azdias = azdias_subset.isnull().sum()\n",
    "#nan_count_column_azdias.sort_values(ascending=False)\n",
    "\n",
    "# Plot the distribution of missing or unknown data for each column\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Distribution of Missing Data in Each Column')\n",
    "plt.hist(nan_count_column_azdias, bins=100)\n",
    "plt.ylabel('Number of Columns')\n",
    "plt.xlabel('Number of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** There are 891221 rows in the `azdias` dataset. If more than 200000 rows have missing values in a column, it indicates two or more rows are missing for every five rows. It will be hard to extract any significant information from these columns. Therefore, these columns will be dropped. Similar approach will be taken for rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns that are needed to be dropped\n",
    "drop_columns = nan_count_column_azdias[nan_count_column_azdias > 200000]\n",
    "\n",
    "# Make list of the drop_columns\n",
    "drop_column_list = drop_columns.index.tolist()\n",
    "\n",
    "# Drop the columns from azdias\n",
    "azdias_subset.drop(columns=drop_column_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows that are needed to be dropped\n",
    "count_nan_row = azdias_subset.isnull().sum(axis=1)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Distribution of Missing Data in Each Row')\n",
    "plt.hist(count_nan_row, bins=50)\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.xlabel('Number of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows where more than 50 values are missing or unknown\n",
    "count_nan_row = azdias_subset.shape[1] - azdias_subset.count(axis=1)\n",
    "drop_row = azdias_subset.index[count_nan_row > 50]\n",
    "azdias_subset.drop(drop_row, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset.to_csv(\"azdias_subset_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset = pd.read_csv(\"azdias_subset_clean.csv\")\n",
    "azdias_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['AGER_TYP', 'ANREDE_KZ', 'BIP_FLAG', 'CAMEO_DEU_2015', 'CAMEO_DEUINTL_2015', 'CJT_GESAMTTYP',\n",
    "'D19_KONSUMTYP', 'D19_KK_KUNDENTYP', 'FINANZTYP', 'GEBAEUDETYP', 'GEBAEUDETYP_RASTER', 'GFK_URLAUBERTYP',\n",
    "'HAUSHALTSSTRUKTUR', 'HEALTH_TYP', 'KBA05_HERSTTEMP', 'KBA05_MAXHERST', 'KBA05_MODTEMP', 'LP_FAMILIE_FEIN',\n",
    "'LP_FAMILIE_GROB', 'LP_LEBENSPHASE_FEIN', 'LP_LEBENSPHASE_GROB', 'LP_STATUS_FEIN', 'LP_STATUS_GROB',\n",
    "'NATIONALITAET_KZ', 'OST_WEST_KZ', 'PRAEGENDE_JUGENDJAHRE', 'SHOPPER_TYP', 'SOHO_FLAG', 'TITEL_KZ', 'VERS_TYP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# let's go through categorial columns and check what unique values they have in each columns\n",
    "for i in categorical:\n",
    "    if i in azdias_subset.columns:\n",
    "        print(i, len(azdias_subset[i].value_counts()), azdias_subset[i].unique().tolist())\n",
    "    else:\n",
    "        print(i + \" dropped already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset[\"OST_WEST_KZ\"].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing characters with 1 and 0\n",
    "azdias_subset['OST_WEST_KZ'].replace(['W', 'O'], [1.0, 0.0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset['OST_WEST_KZ'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop fine features, rough features are still available\n",
    "azdias_subset.drop(['CAMEO_DEU_2015', 'LP_FAMILIE_FEIN', 'LP_STATUS_FEIN'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset[[\"GEBURTSJAHR\", \"GREEN_AVANTGARDE\", \"PRAEGENDE_JUGENDJAHRE\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pj_to_dec(val):\n",
    "    \"\"\"\n",
    "    Converts value of feature PRAEGENDE_JUGENDJAHRE to a DECADE value.\n",
    "\n",
    "    INPUT:\n",
    "    - val (int): original value\n",
    "\n",
    "    OUTPUT:\n",
    "    - int: converted value (0: 40s, 1: 50s, 2: 60s, 3: 70s, 4: 80s, 5: 90s)\n",
    "    \"\"\"\n",
    "    \n",
    "    result = val\n",
    "    if (val > 0 and val <= 2):\n",
    "        result = 0 #40s\n",
    "    elif (val <= 4):\n",
    "        result = 1 #50s\n",
    "    elif (val <= 7):\n",
    "        result = 2 #60s\n",
    "    elif (val <= 9):\n",
    "        result = 3 #70s\n",
    "    elif (val <= 13):\n",
    "        result = 4 #80s\n",
    "    elif (val <= 15):\n",
    "        result = 5 #90\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pj_to_move(val):\n",
    "    \"\"\"\n",
    "    Converts value of feature PRAEGENDE_JUGENDJAHRE to a MOVEMENT value.\n",
    "\n",
    "    INPUT:\n",
    "    - val (int): original value\n",
    "\n",
    "    OUTPUT:\n",
    "    - int: converted value (0: Mainstream, 1: Avantgarde)\n",
    "    \"\"\"\n",
    "    \n",
    "    result = val\n",
    "    if (val in [1,3,5,8,10,12,14]):\n",
    "        result = 0 # M\n",
    "    elif (val in [2,4,6,7,9,11,13,15]):\n",
    "        result = 1 # A\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate \"PRAEGENDE_JUGENDJAHRE\" and engineer two new variables.\n",
    "azdias_subset['DECADE'] = azdias_subset['PRAEGENDE_JUGENDJAHRE'].apply(lambda x: convert_pj_to_dec(x))\n",
    "azdias_subset['MOVEMENT'] = azdias_subset['PRAEGENDE_JUGENDJAHRE'].apply(lambda x: convert_pj_to_move(x))\n",
    "\n",
    "# drop PRAEGENDE_JUGENDJAHRE\n",
    "azdias_subset.drop('PRAEGENDE_JUGENDJAHRE', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop LP_LEBENSPHASE_FEIN and LP_LEBENSPHASE_GROB because no clear structure available\n",
    "azdias_subset.drop(['LP_LEBENSPHASE_FEIN', 'LP_LEBENSPHASE_GROB'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# impute median and scale azdias\n",
    "azdias_imputed = pd.DataFrame(imputer.fit_transform(azdias_subset))\n",
    "azdias_scaled = scaler.fit_transform(azdias_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(azdias_filepath, customers_filepath, attributes_filepath):\n",
    "    \"\"\"\n",
    "    Method for loading dataset from CSV & Excel\n",
    "    \n",
    "    Args:\n",
    "        azdias_filepath (str): Azdias Filepath\n",
    "        customers_filepath (str): Customers Filepath\n",
    "        attributes_filepath (str): Attributes Filepath\n",
    "        \n",
    "    Output:\n",
    "        azdias: Pandas Dataframe\n",
    "        customers: Pandas Dataframe\n",
    "        attributes: Pandas Dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load \"azdias\" dataset\n",
    "    azdias = pd.read_csv(azdias_filepath, na_values=[\"X\", \"XX\"], engine=\"c\")\n",
    "    \n",
    "    # Load \"customers\" dataset\n",
    "    customers = pd.read_csv(customers_filepath, na_values=[\"X\", \"XX\"], engine=\"c\")\n",
    "    \n",
    "    # Load \"attributes\" dataset\n",
    "    attributes = pd.read_excel(\"DIAS Attributes - Values 2017.xlsx\", header=1).loc[:, [\"Attribute\", \"Value\", \"Meaning\"]] \\\n",
    "                                .fillna(method='ffill')\n",
    "    \n",
    "    return azdias, customers, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_filepath = \"Udacity_AZDIAS_052018.csv\"\n",
    "customers_filepath = \"Udacity_CUSTOMERS_052018.csv\"\n",
    "attributes_filepath = \"DIAS Attributes - Values 2017.xlsx\"\n",
    "\n",
    "azdias, customers, attributes = load_data(azdias_filepath, customers_filepath, attributes_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Value</th>\n",
       "      <th>Meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>0</td>\n",
       "      <td>no classification possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>1</td>\n",
       "      <td>passive elderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>2</td>\n",
       "      <td>cultural elderly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AGER_TYP</td>\n",
       "      <td>3</td>\n",
       "      <td>experience-driven elderly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Attribute Value                     Meaning\n",
       "0  AGER_TYP    -1                     unknown\n",
       "1  AGER_TYP     0  no classification possible\n",
       "2  AGER_TYP     1             passive elderly\n",
       "3  AGER_TYP     2            cultural elderly\n",
       "4  AGER_TYP     3   experience-driven elderly"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, attributes, col_nan_threshold, row_nan_threshold):\n",
    "    \"\"\"\n",
    "    Method for cleaning the dataset according to attributes\n",
    "    \n",
    "    Args:\n",
    "        df (Pandas Dataframe): Dataset to clean\n",
    "        attributes (Pandas Dataframe): Reference dataset for cleaning df\n",
    "        col_nan_threshold (float): Threshold value (0-1) for missing NaN count in a column\n",
    "        \n",
    "    Output:\n",
    "        df (Pandas Dataframe): Cleaned dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of attributes\n",
    "    attribute_list = list(attributes.Attribute.unique())\n",
    "    \n",
    "    # List of df attributes\n",
    "    df_attributes = list(df.columns)\n",
    "    \n",
    "    # Find attributes that are unique in df\n",
    "    unique_df_attributes = list(set(df_attributes) - set(attribute_list))    \n",
    "    \n",
    "    # Drop attributes from df that are not present in attribute\n",
    "    df.drop(columns=unique_df_attributes, inplace=True)\n",
    "    \n",
    "    # Create a subset of attributes with unknown or no information value\n",
    "    unknown_val = []\n",
    "    attributes_unknown_val = attributes[(attributes['Meaning'].str.contains(\"unknown\") | \n",
    "                                         attributes['Meaning'].str.contains(\"no \"))]\n",
    "    for attribute in attributes_unknown_val['Attribute'].unique():\n",
    "        val = attributes_unknown_val.loc[attributes['Attribute'] == attribute, 'Value'].astype(\"str\") \\\n",
    "                                                                            .str.cat(sep=',').split(',')\n",
    "        val = list(map(int, val))\n",
    "        unknown_val.append(val)\n",
    "    attributes_unknown_val = pd.concat([pd.Series(attributes_unknown_val['Attribute'].unique()),\n",
    "                                        pd.Series(unknown_val)], axis=1)\n",
    "    attributes_unknown_val.columns = ['attribute', 'unknown']\n",
    "    \n",
    "    # Replace unknown or missing values in df with NaN   \n",
    "    for row in attributes_unknown_val.itertuples(index=False):\n",
    "        if row.attribute in df.columns.values.tolist():\n",
    "            nan_val = attributes_unknown_val.loc[attributes_unknown_val['attribute'] == row.attribute, 'unknown'].iloc[0]\n",
    "            nan_idx = df.loc[:, row.attribute].isin(nan_val)\n",
    "            df.loc[nan_idx, row.attribute] = np.NaN\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Drop columns with NaN count above the threshold\n",
    "    nan_count_column = (df.isnull().sum()/df.shape[0]).sort_values(ascending=False)\n",
    "    # Select the columns that are needed to be dropped\n",
    "    nan_count_column = nan_count_column[nan_count_column > col_nan_threshold]\n",
    "    # Make list of the drop_columns\n",
    "    drop_column_list = nan_count_column.index.tolist()\n",
    "    # Drop the columns from azdias\n",
    "    df.drop(columns=drop_column_list, inplace=True)\n",
    "    \n",
    "    # Dropping rows where more than 50 values are missing or unknown\n",
    "    number_col = df.shape[1]\n",
    "    count_nan_row = (df.shape[1] - df.count(axis=1)) / number_col\n",
    "    drop_row = df.index[count_nan_row > row_nan_threshold]\n",
    "    df.drop(drop_row, axis=0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANZ_HAUSHALTE_AKTIV</th>\n",
       "      <th>ANZ_HH_TITEL</th>\n",
       "      <th>ANZ_PERSONEN</th>\n",
       "      <th>ANZ_TITEL</th>\n",
       "      <th>BALLRAUM</th>\n",
       "      <th>CAMEO_DEU_2015</th>\n",
       "      <th>CAMEO_DEUG_2015</th>\n",
       "      <th>CJT_GESAMTTYP</th>\n",
       "      <th>EWDICHTE</th>\n",
       "      <th>FINANZ_ANLEGER</th>\n",
       "      <th>...</th>\n",
       "      <th>SEMIO_TRADV</th>\n",
       "      <th>SEMIO_VERT</th>\n",
       "      <th>SHOPPER_TYP</th>\n",
       "      <th>VERS_TYP</th>\n",
       "      <th>W_KEIT_KIND_HH</th>\n",
       "      <th>WOHNDAUER_2008</th>\n",
       "      <th>WOHNLAGE</th>\n",
       "      <th>ZABEOTYP</th>\n",
       "      <th>ANREDE_KZ</th>\n",
       "      <th>ALTERSKATEGORIE_GROB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8A</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4C</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6B</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8C</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ANZ_HAUSHALTE_AKTIV  ANZ_HH_TITEL  ANZ_PERSONEN  ANZ_TITEL  BALLRAUM  \\\n",
       "1                 11.0           0.0           2.0        0.0       6.0   \n",
       "2                 10.0           0.0           1.0        0.0       2.0   \n",
       "3                  1.0           0.0           0.0        0.0       4.0   \n",
       "4                  3.0           0.0           4.0        0.0       2.0   \n",
       "5                  5.0           0.0           1.0        0.0       6.0   \n",
       "\n",
       "  CAMEO_DEU_2015  CAMEO_DEUG_2015  CJT_GESAMTTYP  EWDICHTE  FINANZ_ANLEGER  \\\n",
       "1             8A              8.0            5.0       3.0             5.0   \n",
       "2             4C              4.0            3.0       4.0             2.0   \n",
       "3             2A              2.0            2.0       2.0             2.0   \n",
       "4             6B              6.0            5.0       5.0             1.0   \n",
       "5             8C              8.0            2.0       2.0             2.0   \n",
       "\n",
       "   ...  SEMIO_TRADV  SEMIO_VERT  SHOPPER_TYP  VERS_TYP  W_KEIT_KIND_HH  \\\n",
       "1  ...          6.0         1.0          3.0       2.0             3.0   \n",
       "2  ...          3.0         4.0          2.0       1.0             3.0   \n",
       "3  ...          4.0         4.0          1.0       1.0             NaN   \n",
       "4  ...          2.0         7.0          2.0       2.0             2.0   \n",
       "5  ...          6.0         2.0          0.0       2.0             6.0   \n",
       "\n",
       "   WOHNDAUER_2008  WOHNLAGE  ZABEOTYP  ANREDE_KZ  ALTERSKATEGORIE_GROB  \n",
       "1             9.0       4.0       5.0        2.0                   1.0  \n",
       "2             9.0       2.0       5.0        2.0                   3.0  \n",
       "3             9.0       7.0       3.0        2.0                   4.0  \n",
       "4             9.0       3.0       4.0        1.0                   3.0  \n",
       "5             9.0       7.0       4.0        2.0                   1.0  \n",
       "\n",
       "[5 rows x 237 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_data(azdias, attributes, 0.2, 0.2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of categorical variable\n",
    "category_list = ['ANREDE_KZ', 'CJT_GESAMTTYP', 'FINANZTYP', 'GFK_URLAUBERTYP', 'GREEN_AVANTGARDE', 'LP_FAMILIE_FEIN',\n",
    "                 'LP_FAMILIE_GROB', 'LP_STATUS_FEIN', 'LP_STATUS_GROB', 'NATIONALITAET_KZ', 'SHOPPER_TYP', 'SOHO_KZ',\n",
    "                 'VERS_TYP', 'ZABEOTYP', 'GEBAEUDETYP', 'OST_WEST_KZ', 'CAMEO_DEUG_2015', 'CAMEO_DEU_2015', 'D19_KONSUMTYP',\n",
    "                 'KBA05_MAXHERST', 'KBA05_MAXSEG']\n",
    "len(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer – this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
